# Attention-based Models

Biologically-oriented vs pragmatic models of attention, Transformer revolution, the Consciousness prior, etc.

## Intro

The immediate impulse for this subdirectory is the next stage of Transformer revolution, the qualitative shift
associated with GPT-3 and OpenAI API, which started publicly two months ago, on May 20, 2020:

https://www.cs.brandeis.edu/~bukatin/transformer_revolution.html

This is "the AlexNet moment" of the Transformer revolution, and we should expect that people will
hybridize all kinds of things with Transformers and other attention-based models in the near future,
just like they hybridized all kinds of approaches with "Deep Nets" in recent years, obtaining
a variety of architectures such as Deep Probabilistic Programming, Deep Neuroevolution, etc.

