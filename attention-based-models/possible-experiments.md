# Possible attention-related experiments

## Adding feedback to Transformers

Something in the style of "Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention over Modules",
https://arxiv.org/abs/2006.16981 , but that paper is only doing it inside a two-layer system which is not good enough. Instead, one
should take a small-scale easy-to-train version of a Transformer, and see what adding feedback modulating connections might do.

