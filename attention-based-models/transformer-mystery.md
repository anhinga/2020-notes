# Transformer Mystery: the Unreasonable Effectiveness of Multilayer Attention Models

Transformers delivered more than they promised, and more than one could have expected at the first glance.

The original motivation of Transformer models was two-fold: easy modelling of long-range dependencies and
nice parallelization. That's very cool, but why on Earth does this result in the ability to synthesize
parse trees (e.g. "Visualizing and Measuring the Geometry of BERT", https://arxiv.org/abs/1906.02715)?
